{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of baseline using Artificial Neural Network with Original + Win-loss feature set and BL"
      ],
      "metadata": {
        "id": "8G547Yd-gkhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necessary files: logistic_regression_functions.py, ann_functions.py, testing_functions.py, win_loss_functions.py, BL_17.csv, BL_18.csv, BL_19.csv"
      ],
      "metadata": {
        "id": "3KqOnAnMdqng"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxVhEXIolv7H"
      },
      "outputs": [],
      "source": [
        "import logistic_regression_functions\n",
        "import ann_functions\n",
        "import testing_functions\n",
        "import win_loss_functions\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import requests\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import *\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The baseline is implemented according to the paper An Improved Prediction System for Football a Match Result bu C. P. Igiri and E. O. Nwachukwu from 2014. The original features as described in the paper are used and the model is evaluated on the BL dataset."
      ],
      "metadata": {
        "id": "gveMzlcOguP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The authors used only one season for training, so I did the same. I used seasons 2016/17 for training season 2017/18 as validation set and the second half of season 2018/19 for testing. The validation set is used to select the best checkpoint of the training according to the validation accuracy."
      ],
      "metadata": {
        "id": "ba1tAiVgxBmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_win_loss, y_train = win_loss_functions.create_data(['BL_17.csv'], skip_rounds = 6)\n",
        "results_val_win_loss, matches_per_round = win_loss_functions.create_data_single('BL_18.csv', ['BL_17.csv'], skip_rounds = 6)\n",
        "# Dates are returned as well for dividing testing season into slices\n",
        "results_test_win_loss, matches_per_round = win_loss_functions.create_data_single('BL_19.csv', ['BL_17.csv', 'BL_18.csv'],\n",
        "                                return_dates=True, skip_rounds = 6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvVsb_YNl_S6",
        "outputId": "49c520fb-bc6e-413e-cad2-9a98fca5dd93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing BL_17.csv season file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_train_originals, matches_per_round = ann_functions.create_data_single('BL_17.csv', 19, 17,\n",
        "                                   logistic_regression_functions.team_names_map_bl,\n",
        "                                   logistic_regression_functions.secondary_team_names_map_bl)\n",
        "results_val_originals, matches_per_round = ann_functions.create_data_single('BL_18.csv', 19, 18,\n",
        "                                  logistic_regression_functions.team_names_map_bl,\n",
        "                                  logistic_regression_functions.secondary_team_names_map_bl)\n",
        "# Dates are returned as well for dividing testing season into slices\n",
        "results_test_originals, matches_per_round = ann_functions.create_data_single('BL_19.csv', 19, 19,\n",
        "                                  logistic_regression_functions.team_names_map_bl,\n",
        "                                  logistic_regression_functions.secondary_team_names_map_bl,\n",
        "                                  return_dates=True)\n",
        "X_train_originals = results_train_originals.drop('FTR', axis=1)\n",
        "y_train_originals = results_train_originals['FTR']"
      ],
      "metadata": {
        "id": "Kk6Yj2YGmq_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenating the two feature sets together\n",
        "X_train = pd.concat([X_train_originals, X_train_win_loss], axis=1)\n",
        "results_val = pd.concat([results_val_originals, results_val_win_loss], axis=1)\n",
        "results_test = pd.concat([results_test_originals, results_test_win_loss], axis=1)\n",
        "X_train = X_train.loc[:,~X_train.columns.duplicated()].copy()\n",
        "results_val = results_val.loc[:,~results_val.columns.duplicated()].copy()\n",
        "results_test = results_test.loc[:,~results_test.columns.duplicated()].copy()"
      ],
      "metadata": {
        "id": "dMBhL98fm6RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val = results_val.drop('FTR', axis=1)\n",
        "y_val = results_val['FTR']\n",
        "# The classes need to go from 0 to 2 not from -1 to 1.\n",
        "y_train += 1\n",
        "y_val += 1"
      ],
      "metadata": {
        "id": "_HVok9KBn-s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "They didnt say what ANN architecture they used. I used 3 dense layers with 64, 32 and 16 neurons. I used dropout to overcome overfitting."
      ],
      "metadata": {
        "id": "YXiGk_-1hhbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I always let the model train for 100 epochs and took the weights from the best epoch according to validation accuracy."
      ],
      "metadata": {
        "id": "uKcgCSFJhlMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use all the data available, the first half of the testing season was added to the training data."
      ],
      "metadata": {
        "id": "0yyeco6Ehov-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some rounds in the beginning are ignored, this is the correct index\n",
        "# of the start of the second half of the season\n",
        "start_test_index = 11 * matches_per_round"
      ],
      "metadata": {
        "id": "KdpfY51fbble"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_to_append, y_test_to_append = testing_functions.prepare_test_to_append(results_test,\n",
        "                                                                              start_test_index)\n",
        "y_test_to_append += 1"
      ],
      "metadata": {
        "id": "4zH4fVuybc-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For evaluation on the testing dataset I divided the testing data into rounds approximately. The model is trained on the training dataset, then it is evaluated on one round of the testing dataset and this round is added to the training dataset."
      ],
      "metadata": {
        "id": "lW5XExkEhuge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding 1st half of testing season to the training data\n",
        "X_train = pd.concat([X_train, X_test_to_append])\n",
        "y_train = pd.concat([y_train, y_test_to_append])\n",
        "# Rounds of the testing dataset\n",
        "slices = testing_functions.get_slices(results_test, matches_per_round,\n",
        "                                      start_test_index)\n",
        "weighted_sum = 0\n",
        "sum = 0\n",
        "for slc in tqdm(slices):\n",
        "  # Creating the model\n",
        "  model = ann_functions.func_model((X_train.shape[1],))\n",
        "  model.compile(optimizer='adam',\n",
        "                 loss='sparse_categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "  mc = tf.keras.callbacks.ModelCheckpoint('./weights_model.h5',\n",
        "                                     monitor='val_accuracy',\n",
        "                                     save_weights_only=True,\n",
        "                                     save_best_only=True)\n",
        "  X_test = slc.drop(['FTR', 'Date'], axis=1)\n",
        "  y_test = slc['FTR']\n",
        "  y_test += 1\n",
        "  # Train the model\n",
        "  history = model.fit(X_train, y_train,\n",
        "                    epochs=100,\n",
        "                    batch_size=8,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose = 0,\n",
        "                    callbacks=[mc])\n",
        "  # Load the best checkpoint\n",
        "  model.load_weights('weights_model.h5')\n",
        "  weighted_sum += (model.evaluate(X_test, y_test)[1] * len(y_test))\n",
        "  sum += len(y_test)\n",
        "  # Add the round to the training dataset\n",
        "  X_train = pd.concat([X_train, X_test])\n",
        "  y_train = pd.concat([y_train, y_test])\n",
        "print('')\n",
        "print(weighted_sum / sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvn2jFT4o5Tt",
        "outputId": "75de8195-a466-4051-8462-93a12ce9ca95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/13 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step - loss: 0.8320 - accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 1/13 [00:21<04:15, 21.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9404 - accuracy: 0.5333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 2/13 [00:38<03:28, 18.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step - loss: 1.1435 - accuracy: 0.4545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 3/13 [00:56<03:04, 18.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9688 - accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███       | 4/13 [01:13<02:42, 18.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7875 - accuracy: 0.6667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 5/13 [01:31<02:24, 18.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7022 - accuracy: 0.7000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 6/13 [01:50<02:06, 18.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step - loss: 0.8383 - accuracy: 0.7000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 7/13 [02:08<01:48, 18.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step - loss: 1.2933 - accuracy: 0.3000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 8/13 [02:26<01:30, 18.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step - loss: 0.8481 - accuracy: 0.6429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69%|██████▉   | 9/13 [02:47<01:16, 19.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step - loss: 0.8143 - accuracy: 0.7273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 77%|███████▋  | 10/13 [03:07<00:58, 19.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 29ms/step - loss: 1.7292 - accuracy: 0.1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▍ | 11/13 [03:27<00:38, 19.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 28ms/step - loss: 1.0509 - accuracy: 0.4118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 12/13 [03:47<00:19, 19.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step - loss: 1.0045 - accuracy: 0.6000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [04:08<00:00, 19.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0.549019616511133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The testing accuracy is 54.90%."
      ],
      "metadata": {
        "id": "APh6XV83iVqU"
      }
    }
  ]
}