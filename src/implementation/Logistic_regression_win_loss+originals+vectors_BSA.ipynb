{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of baseline using Logistic Regression with Original + Win-loss + feature vectors feature set and BSA"
      ],
      "metadata": {
        "id": "FGZA_YmOAORv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necessary files: win_loss_functions.py, logistic_regression_functions.py, testing_functions.py, feature_vectors_functions.py, BRA.csv, A_BSA.csv, H_BSA.csv"
      ],
      "metadata": {
        "id": "GdwLOHZ6j4qg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ds6xcyupbuPr"
      },
      "outputs": [],
      "source": [
        "import win_loss_functions\n",
        "import logistic_regression_functions\n",
        "import testing_functions\n",
        "import feature_vectors_functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The baseline is implemented according to the paper Predicting Football Match Results with Logistic Regression by D. Prasetio and M. Harlili from 2016. There was a lot of missing data in the original features, so features based only on results of previous matches and feature vectors are used and the model is evaluated on the BSA dataset."
      ],
      "metadata": {
        "id": "nN9vK955A6bU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The authors had the best results with 5 training seasons, so I used 5 seasons for training as well. I used seasons from 2012/13 to 2016/17 for training, season 2017/18 as validation set and the second half of season 2018/19 for testing. The validation set is used to find the best value for the treshold, which defines, when are draws predicted."
      ],
      "metadata": {
        "id": "GrZyGac-Boe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset = pd.read_csv('BRA.csv')\n",
        "full_dataset.rename(columns = {'Home':'HomeTeam', 'Away': 'AwayTeam',\n",
        "                               'HG': 'FTHG', 'AG': 'FTAG', 'Res': 'FTR',\n",
        "                               'PH': 'B365H', 'PD': 'B365D',\n",
        "                               'PA': 'B365A'}, inplace = True)\n",
        "for season in full_dataset['Season'].unique():\n",
        "  dataset = full_dataset[full_dataset['Season'] == season]\n",
        "  dataset.to_csv('BSA_' + str(season)[-2:] + '.csv', index=False)"
      ],
      "metadata": {
        "id": "pNgHLAp-pAs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The draws are dropped for training, but not for validation and testing\n",
        "X_train_win_loss, y_train = win_loss_functions.create_data(['BSA_17.csv', 'BSA_16.csv', 'BSA_15.csv',\n",
        "                                'BSA_14.csv', 'BSA_13.csv'],\n",
        "                                drop_draws=True, return_names=True)\n",
        "results_val_win_loss, matches_per_round = win_loss_functions.create_data_single('BSA_18.csv', ['BSA_13.csv', 'BSA_14.csv', 'BSA_15.csv',\n",
        "                                'BSA_16.csv', 'BSA_17.csv'], return_names=True)\n",
        "# Dates are returned as well for dividing testing season into slices\n",
        "results_test_win_loss, matches_per_round = win_loss_functions.create_data_single('BSA_19.csv', ['BSA_13.csv', 'BSA_14.csv', 'BSA_15.csv',\n",
        "                                'BSA_16.csv', 'BSA_17.csv', 'BSA_18.csv'],\n",
        "                                return_dates=True, return_names=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtTjg67QcAda",
        "outputId": "6418ba1a-13ef-477c-88b5-8963363a1f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing BSA_17.csv season file.\n",
            "Processing BSA_16.csv season file.\n",
            "Processing BSA_15.csv season file.\n",
            "Processing BSA_14.csv season file.\n",
            "Processing BSA_13.csv season file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Originaly the results are represented as 1 for home win, 0 for draw and -1 for\n",
        "# away win, but Logistic Regression predicts numbers between 0 and 1, so home win\n",
        "# is still represented as 1, but away win as 0 and draw as 2.\n",
        "y_train.replace(-1, 0, inplace=True)\n",
        "results_val_win_loss.replace(0, 2, inplace=True)\n",
        "results_val_win_loss.replace(-1, 0, inplace=True)\n",
        "results_test_win_loss.replace(0, 2, inplace=True)\n",
        "results_test_win_loss.replace(-1, 0, inplace=True)"
      ],
      "metadata": {
        "id": "LiaYccQOcD1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = pd.read_csv('A_BSA.csv')\n",
        "H = pd.read_csv('H_BSA.csv')"
      ],
      "metadata": {
        "id": "2ev7PQeBciI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the feature vectors to the features\n",
        "X_train = feature_vectors_functions.add_feature_vector(X_train_win_loss, A, H)\n",
        "results_val = feature_vectors_functions.add_feature_vector(results_val_win_loss, A, H)\n",
        "results_test = feature_vectors_functions.add_feature_vector(results_test_win_loss, A, H)"
      ],
      "metadata": {
        "id": "T-CbXcYDc36M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best value for the treshold is found with the validation season. For each value of treshold is the model trained on the training data and evaluated on the validation dataset. The treshold with the best validation accuracy is chosen."
      ],
      "metadata": {
        "id": "ySrfzDXpCwBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_val = results_val.drop('FTR', axis=1)\n",
        "y_val = results_val['FTR']\n",
        "clf = LogisticRegression(random_state=42, max_iter=10000).fit(X_train, y_train)\n",
        "best_score = 0\n",
        "best_threshold = 0\n",
        "for threshold in [0, 0.0125, 0.025, 0.0375, 0.05, 0.0625, 0.075, 0.0875, 0.1, 0.1125, 0.125]:\n",
        "  preds = clf.predict_proba(X_val)[:, 1]\n",
        "  score = logistic_regression_functions.evaluate(preds, y_val, threshold)\n",
        "  print(\"threshold \", threshold, \" score \", score)\n",
        "  if score > best_score:\n",
        "    best_score = score\n",
        "    best_threshold = threshold\n",
        "print(\"best threshold \", best_threshold, \" best score \", best_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR6iaAA8c-VS",
        "outputId": "6f86a3fc-a858-4a05-c873-a33671503cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "threshold  0  score  0.5263157894736843\n",
            "threshold  0.0125  score  0.5263157894736843\n",
            "threshold  0.025  score  0.5184210526315789\n",
            "threshold  0.0375  score  0.5105263157894737\n",
            "threshold  0.05  score  0.5078947368421053\n",
            "threshold  0.0625  score  0.5\n",
            "threshold  0.075  score  0.49473684210526314\n",
            "threshold  0.0875  score  0.4921052631578947\n",
            "threshold  0.1  score  0.4921052631578947\n",
            "threshold  0.1125  score  0.4921052631578947\n",
            "threshold  0.125  score  0.4894736842105263\n",
            "best threshold  0  best score  0.5263157894736843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best treshold is 0."
      ],
      "metadata": {
        "id": "MJ9sndHXC4O4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use all the data available, the validation season and the first half of the testing season was added to the training data."
      ],
      "metadata": {
        "id": "syS7HWbrDCB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_to_append, y_test_to_append = testing_functions.prepare_test_to_append(results_test,\n",
        "                                                                              int(results_test.shape[0] / 2),\n",
        "                                                                              drop_draws=True)"
      ],
      "metadata": {
        "id": "mpowetua_ee0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For evaluation on the testing dataset I divided the testing data into rounds approximately. The model is trained on the training dataset, then it is evaluated on one round of the testing dataset and this round is added to the training dataset."
      ],
      "metadata": {
        "id": "t8wYCY51DIiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding validation season and 1st half of testing season to the training data\n",
        "X_train = pd.concat([X_train, X_val, X_test_to_append])\n",
        "y_train = pd.concat([y_train, y_val, y_test_to_append])\n",
        "# Rounds of the testing dataset\n",
        "slices = testing_functions.get_slices(results_test, matches_per_round,\n",
        "                                      int(results_test.shape[0] / 2))\n",
        "weighted_sum = 0\n",
        "sum = 0\n",
        "for slc in slices:\n",
        "  # Ignore draws for training\n",
        "  test_to_append = slc[slc['FTR'] != 2]\n",
        "  clf = LogisticRegression(random_state=42, max_iter=10000).fit(X_train, y_train)\n",
        "  X_test = slc.drop(['FTR', 'Date'], axis=1)\n",
        "  y_test = slc['FTR']\n",
        "  preds = clf.predict_proba(X_test)[:, 1]\n",
        "  weighted_sum += (logistic_regression_functions.evaluate(preds, y_test, best_threshold) * len(y_test))\n",
        "  sum += len(y_test)\n",
        "  # Add the round to the training dataset\n",
        "  X_test = test_to_append.drop(['FTR', 'Date'], axis=1)\n",
        "  y_test = test_to_append['FTR']\n",
        "  X_train = pd.concat([X_train, X_test])\n",
        "  y_train = pd.concat([y_train, y_test])\n",
        "  #print(evaluate(preds, y_test, best_threshold) * len(y_test))\n",
        "print(weighted_sum / sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuG_aZE4dHoV",
        "outputId": "737d50b1-1c7b-4811-9079-0ccde76adbe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4842105263157895\n"
          ]
        }
      ]
    }
  ]
}