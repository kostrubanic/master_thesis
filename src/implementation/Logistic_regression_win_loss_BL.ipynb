{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of baseline using Logistic Regression with Win-loss feature set and BL"
      ],
      "metadata": {
        "id": "dAVEE699Za-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necessary files: win_loss_functions.py, logistic_regression_functions.py, testing_functions.py, BL_13.csv, BL_14.csv, ..., BL_19.csv"
      ],
      "metadata": {
        "id": "-q_SqsQ8gF3v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdVbaSs7uRxg"
      },
      "outputs": [],
      "source": [
        "import win_loss_functions\n",
        "import logistic_regression_functions\n",
        "import testing_functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The baseline is implemented according to the paper Predicting Football Match Results with Logistic Regression by D. Prasetio and M. Harlili from 2016. The features based only on results of previous matches are used and the model is evaluated on the BL dataset."
      ],
      "metadata": {
        "id": "yj3wpWfuZouW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The authors had the best results with 5 training seasons, so I used 5 seasons for training as well. I used seasons from 2012/13 to 2016/17 for training, season 2017/18 as validation set and the second half of season 2018/19 for testing. The validation set is used to find the best value for the treshold, which defines, when are draws predicted."
      ],
      "metadata": {
        "id": "04IiahQlZzqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The draws are dropped for training, but not for validation and testing\n",
        "X_train, y_train = win_loss_functions.create_data(['BL_17.csv', 'BL_16.csv',\n",
        "                                'BL_15.csv', 'BL_14.csv', 'BL_13.csv'],\n",
        "                                drop_draws=True)\n",
        "results_val, matches_per_round = win_loss_functions.create_data_single('BL_18.csv', ['BL_13.csv', 'BL_14.csv',\n",
        "                                'BL_15.csv', 'BL_16.csv', 'BL_17.csv'])\n",
        "# Dates are returned as well for dividing testing season into slices\n",
        "results_test, matches_per_round = win_loss_functions.create_data_single('BL_19.csv', ['BL_13.csv', 'BL_14.csv',\n",
        "                                'BL_15.csv', 'BL_16.csv', 'BL_17.csv', 'BL_18.csv'],\n",
        "                                return_dates=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3ws8QUsyL4H",
        "outputId": "36f5cdcf-9e4b-44dd-b500-ee95ce996304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing BL_17.csv season file.\n",
            "Processing BL_16.csv season file.\n",
            "Processing BL_15.csv season file.\n",
            "Processing BL_14.csv season file.\n",
            "Processing BL_13.csv season file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Originaly the results are represented as 1 for home win, 0 for draw and -1 for\n",
        "# away win, but Logistic Regression predicts numbers between 0 and 1, so home win\n",
        "# is still represented as 1, but away win as 0 and draw as 2.\n",
        "y_train.replace(-1, 0, inplace=True)\n",
        "results_val.replace(0, 2, inplace=True)\n",
        "results_val.replace(-1, 0, inplace=True)\n",
        "results_test.replace(0, 2, inplace=True)\n",
        "results_test.replace(-1, 0, inplace=True)"
      ],
      "metadata": {
        "id": "cCUZyVU8yk7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best value for the treshold is found with the validation season. For each value of treshold is the model trained on the training data and evaluated on the validation dataset. The treshold with the best validation accuracy is chosen."
      ],
      "metadata": {
        "id": "csFfEW2GaqGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_val = results_val.drop('FTR', axis=1)\n",
        "y_val = results_val['FTR']\n",
        "clf = LogisticRegression(random_state=42, max_iter=1000).fit(X_train, y_train)\n",
        "best_score = 0\n",
        "best_threshold = 0\n",
        "for threshold in [0, 0.0125, 0.025, 0.0375, 0.05, 0.0625, 0.075, 0.0875, 0.1, 0.1125, 0.125]:\n",
        "  preds = clf.predict_proba(X_val)[:, 1]\n",
        "  score = logistic_regression_functions.evaluate(preds, y_val, threshold)\n",
        "  print(\"threshold \", threshold, \" score \", score)\n",
        "  if score > best_score:\n",
        "    best_score = score\n",
        "    best_threshold = threshold\n",
        "print(\"best threshold \", best_threshold, \" best score \", best_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V37gKj7lyn6D",
        "outputId": "627c0883-55c1-4d81-ab37-f497d7ce568a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "threshold  0  score  0.41503267973856206\n",
            "threshold  0.0125  score  0.4248366013071896\n",
            "threshold  0.025  score  0.43137254901960786\n",
            "threshold  0.0375  score  0.4183006535947712\n",
            "threshold  0.05  score  0.39869281045751637\n",
            "threshold  0.0625  score  0.3921568627450981\n",
            "threshold  0.075  score  0.3758169934640523\n",
            "threshold  0.0875  score  0.36601307189542487\n",
            "threshold  0.1  score  0.369281045751634\n",
            "threshold  0.1125  score  0.35620915032679734\n",
            "threshold  0.125  score  0.34967320261437906\n",
            "best threshold  0.025  best score  0.43137254901960786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best treshold is 0.025."
      ],
      "metadata": {
        "id": "V_evPDPnav4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use all the data available, the validation season and the first half of the testing season was added to the training data."
      ],
      "metadata": {
        "id": "obpJZe5Sa5n8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_to_append, y_test_to_append = testing_functions.prepare_test_to_append(results_test,\n",
        "                                                                              int(results_test.shape[0] / 2),\n",
        "                                                                              drop_draws=True)"
      ],
      "metadata": {
        "id": "wTh9xVVPWrIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For evaluation on the testing dataset I divided the testing data into rounds approximately. The model is trained on the training dataset, then it is evaluated on one round of the testing dataset and this round is added to the training dataset."
      ],
      "metadata": {
        "id": "sCT666USa_7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding validation season and 1st half of testing season to the training data\n",
        "X_train = pd.concat([X_train, X_val, X_test_to_append])\n",
        "y_train = pd.concat([y_train, y_val, y_test_to_append])\n",
        "# Rounds of the testing dataset\n",
        "slices = testing_functions.get_slices(results_test, matches_per_round,\n",
        "                                      int(results_test.shape[0] / 2))\n",
        "weighted_sum = 0\n",
        "sum = 0\n",
        "for slc in slices:\n",
        "  # Ignore draws for training\n",
        "  test_to_append = slc[slc['FTR'] != 2]\n",
        "  clf = LogisticRegression(random_state=42, max_iter=1000).fit(X_train, y_train)\n",
        "  X_test = slc.drop(['FTR', 'Date'], axis=1)\n",
        "  y_test = slc['FTR']\n",
        "  preds = clf.predict_proba(X_test)[:, 1]\n",
        "  weighted_sum += (logistic_regression_functions.evaluate(preds, y_test, best_threshold) * len(y_test))\n",
        "  sum += len(y_test)\n",
        "  # Add the round to the training dataset\n",
        "  X_test = test_to_append.drop(['FTR', 'Date'], axis=1)\n",
        "  y_test = test_to_append['FTR']\n",
        "  X_train = pd.concat([X_train, X_test])\n",
        "  y_train = pd.concat([y_train, y_test])\n",
        "print(weighted_sum / sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3QFM2uAzDz9",
        "outputId": "b47cadd2-ebe1-411a-9c65-52ba19fa26b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.49019607843137253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The testing accuracy is 49.02%"
      ],
      "metadata": {
        "id": "mOZp4su9bTon"
      }
    }
  ]
}