{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of baseline using Logistic Regression with Win-loss feature set and LLPD"
      ],
      "metadata": {
        "id": "vz4SCwdgZd18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necessary files: win_loss_functions.py, logistic_regression_functions.py, testing_functions.py, LLPD_12.csv, LLPD_13.csv, LLPD_15.csv, LLPD_16.csv, ..., LLPD_19.csv"
      ],
      "metadata": {
        "id": "cwpyXTWCghm4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDKYfjFKsKQb"
      },
      "outputs": [],
      "source": [
        "import win_loss_functions\n",
        "import logistic_regression_functions\n",
        "import testing_functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The baseline is implemented according to the paper Predicting Football Match Results with Logistic Regression by D. Prasetio and M. Harlili from 2016. The features based only on results of previous matches are used and the model is evaluated on the LLPD dataset."
      ],
      "metadata": {
        "id": "X82F1qDBZrDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The authors had the best results with 5 training seasons, so I used 5 seasons for training as well. I used seasons 2012/13, 2013/14 and from 2014/15 to 2016/17 for training (because there were some ratings missing in the 2013/14 season), season 2017/18 as validation set and the second half of season 2018/19 for testing. The validation set is used to find the best value for the treshold, which defines, when are draws predicted."
      ],
      "metadata": {
        "id": "P11x6fAoZ79b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The draws are dropped for training, but not for validation and testing\n",
        "X_train, y_train = win_loss_functions.create_data(['LLPD_17.csv', 'LLPD_16.csv', 'LLPD_15.csv',\n",
        "                                'LLPD_13.csv', 'LLPD_12.csv'],\n",
        "                                drop_draws=True)\n",
        "results_val, matches_per_round = win_loss_functions.create_data_single('LLPD_18.csv', ['LLPD_12.csv', 'LLPD_13.csv', 'LLPD_15.csv',\n",
        "                                'LLPD_16.csv', 'LLPD_17.csv'])\n",
        "# Dates are returned as well for dividing testing season into slices\n",
        "results_test, matches_per_round = win_loss_functions.create_data_single('LLPD_19.csv', ['LLPD_12.csv', 'LLPD_13.csv', 'LLPD_15.csv',\n",
        "                                'LLPD_16.csv', 'LLPD_17.csv', 'LLPD_18.csv'],\n",
        "                                return_dates=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljY_lp5VsbAH",
        "outputId": "63ad81c5-b176-4eeb-a81e-a4bc43c2daf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing LLPD_17.csv season file.\n",
            "Processing LLPD_16.csv season file.\n",
            "Processing LLPD_15.csv season file.\n",
            "Processing LLPD_13.csv season file.\n",
            "Processing LLPD_12.csv season file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Originaly the results are represented as 1 for home win, 0 for draw and -1 for\n",
        "# away win, but Logistic Regression predicts numbers between 0 and 1, so home win\n",
        "# is still represented as 1, but away win as 0 and draw as 2.\n",
        "y_train.replace(-1, 0, inplace=True)\n",
        "results_val.replace(0, 2, inplace=True)\n",
        "results_val.replace(-1, 0, inplace=True)\n",
        "results_test.replace(0, 2, inplace=True)\n",
        "results_test.replace(-1, 0, inplace=True)"
      ],
      "metadata": {
        "id": "MaBGCCA-s7Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best value for the treshold is found with the validation season. For each value of treshold is the model trained on the training data and evaluated on the validation dataset. The treshold with the best validation accuracy is chosen."
      ],
      "metadata": {
        "id": "2sEKcVjraqx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_val = results_val.drop('FTR', axis=1)\n",
        "y_val = results_val['FTR']\n",
        "clf = LogisticRegression(random_state=42, max_iter=1000).fit(X_train, y_train)\n",
        "best_score = 0\n",
        "best_threshold = 0\n",
        "for threshold in [0, 0.0125, 0.025, 0.0375, 0.05, 0.0625, 0.075, 0.0875, 0.1, 0.1125, 0.125]:\n",
        "  preds = clf.predict_proba(X_val)[:, 1]\n",
        "  score = logistic_regression_functions.evaluate(preds, y_val, threshold)\n",
        "  print(\"threshold \", threshold, \" score \", score)\n",
        "  if score > best_score:\n",
        "    best_score = score\n",
        "    best_threshold = threshold\n",
        "print(\"best threshold \", best_threshold, \" best score \", best_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jRX3hZQs-Yj",
        "outputId": "e32821cd-3161-4789-aecd-f3d12e02bebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "threshold  0  score  0.4157894736842105\n",
            "threshold  0.0125  score  0.41052631578947374\n",
            "threshold  0.025  score  0.3973684210526316\n",
            "threshold  0.0375  score  0.39210526315789473\n",
            "threshold  0.05  score  0.38421052631578945\n",
            "threshold  0.0625  score  0.38421052631578945\n",
            "threshold  0.075  score  0.381578947368421\n",
            "threshold  0.0875  score  0.3789473684210526\n",
            "threshold  0.1  score  0.38421052631578945\n",
            "threshold  0.1125  score  0.381578947368421\n",
            "threshold  0.125  score  0.3710526315789474\n",
            "best threshold  0  best score  0.4157894736842105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best treshold is 0."
      ],
      "metadata": {
        "id": "W0qItiXtazzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use all the data available, the validation season and the first half of the testing season was added to the training data."
      ],
      "metadata": {
        "id": "vhPITIR2a6uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_to_append, y_test_to_append = testing_functions.prepare_test_to_append(results_test,\n",
        "                                                                              int(results_test.shape[0] / 2),\n",
        "                                                                              drop_draws=True)"
      ],
      "metadata": {
        "id": "Ar5y7mvHXx2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For evaluation on the testing dataset I divided the testing data into rounds approximately. The model is trained on the training dataset, then it is evaluated on one round of the testing dataset and this round is added to the training dataset."
      ],
      "metadata": {
        "id": "At34EvkIbA-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding validation season and 1st half of testing season to the training data\n",
        "X_train = pd.concat([X_train, X_val, X_test_to_append])\n",
        "y_train = pd.concat([y_train, y_val, y_test_to_append])\n",
        "# Rounds of the testing dataset\n",
        "slices = testing_functions.get_slices(results_test, matches_per_round,\n",
        "                                      int(results_test.shape[0] / 2))\n",
        "weighted_sum = 0\n",
        "sum = 0\n",
        "for slc in slices:\n",
        "  # Ignore draws for training\n",
        "  test_to_append = slc[slc['FTR'] != 2]\n",
        "  clf = LogisticRegression(random_state=42, max_iter=1000).fit(X_train, y_train)\n",
        "  X_test = slc.drop(['FTR', 'Date'], axis=1)\n",
        "  y_test = slc['FTR']\n",
        "  preds = clf.predict_proba(X_test)[:, 1]\n",
        "  weighted_sum += (logistic_regression_functions.evaluate(preds, y_test, best_threshold) * len(y_test))\n",
        "  sum += len(y_test)\n",
        "  # Add the round to the training dataset\n",
        "  X_test = test_to_append.drop(['FTR', 'Date'], axis=1)\n",
        "  y_test = test_to_append['FTR']\n",
        "  X_train = pd.concat([X_train, X_test])\n",
        "  y_train = pd.concat([y_train, y_test])\n",
        "print(weighted_sum / sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U2DwRjGtBud",
        "outputId": "abc064dd-a0f3-43fe-a32e-c00d66750cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.45263157894736844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The testing accuracy is 45.26%"
      ],
      "metadata": {
        "id": "_EQKiwQgbW4H"
      }
    }
  ]
}