{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of baseline using PCA and Naive Bayes with Original + Win-loss + feature vectors feature set and BSA"
      ],
      "metadata": {
        "id": "JBYr6Sm47y-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necessary files: testing_functions.py, pca_nb_functions.py, feature_vectors_functions.py, BRA.csv, A_BSA.csv, H_BSA.csv"
      ],
      "metadata": {
        "id": "pBxKzBOnowj2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxuUnLs-fWQ1"
      },
      "outputs": [],
      "source": [
        "import testing_functions\n",
        "import pca_nb_functions\n",
        "import feature_vectors_functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The baseline is implemented according to the paper Predicting The Dutch Football Competition Using Public Data: A Machine Learning Approach by N. Tax and Y. Joustra from 2015. The original features from paper (except for few ones which are not available for the BSA) and feature vectors are used and the model is evaluated on the BSA dataset."
      ],
      "metadata": {
        "id": "PhtyAOw98Alx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The authors used data from 13 seasons. Unfortunaltely, the data for BSA are avilable only from 2012. So I used seasons from 2012 to 2017 for training, season 2018 for validation and the second half of season 2019 for testing. Here the validation part is not necessary, but I wanted to keep the split of the dataset as similar as possible with other baselines and models."
      ],
      "metadata": {
        "id": "bSGUwFMF8Imw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset = pd.read_csv('BRA.csv')\n",
        "full_dataset.rename(columns = {'Home':'HomeTeam', 'Away': 'AwayTeam',\n",
        "                               'HG': 'FTHG', 'AG': 'FTAG', 'Res': 'FTR',\n",
        "                               'PH': 'B365H', 'PD': 'B365D',\n",
        "                               'PA': 'B365A'}, inplace = True)\n",
        "for season in full_dataset['Season'].unique():\n",
        "  dataset = full_dataset[full_dataset['Season'] == season]\n",
        "  dataset.to_csv('BSA_' + str(season)[-2:] + '.csv', index=False)"
      ],
      "metadata": {
        "id": "A55oEc9ZrE3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = pca_nb_functions.create_data(['BSA_17.csv', 'BSA_16.csv',\n",
        "                             'BSA_15.csv', 'BSA_14.csv', 'BSA_13.csv',\n",
        "                             'BSA_12.csv'], pca_nb_functions.teams_bsa, return_names=True,\n",
        "                             include_shots_fauls=False)\n",
        "results_val, matches_per_round = pca_nb_functions.create_data_single('BSA_18.csv', 'BSA_17.csv',\n",
        "                            ['BSA_16.csv',\n",
        "                             'BSA_15.csv', 'BSA_14.csv', 'BSA_13.csv',\n",
        "                             'BSA_12.csv'], pca_nb_functions.teams_bsa, return_names=True,\n",
        "                             include_shots_fauls=False)\n",
        "# Dates are returned as well for dividing testing season into slices\n",
        "results_test, matches_per_round = pca_nb_functions.create_data_single('BSA_19.csv', 'BSA_18.csv',\n",
        "                            ['BSA_17.csv', 'BSA_16.csv',\n",
        "                             'BSA_15.csv', 'BSA_14.csv', 'BSA_13.csv',\n",
        "                             'BSA_12.csv'], pca_nb_functions.teams_bsa,\n",
        "                             return_dates=True, return_names=True,\n",
        "                             include_shots_fauls=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqD7Ua0pgTrm",
        "outputId": "4c190ed7-d699-4398-ee7b-77b22d4317fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing BSA_17.csv season file.\n",
            "Processing BSA_16.csv season file.\n",
            "Processing BSA_15.csv season file.\n",
            "Processing BSA_14.csv season file.\n",
            "Processing BSA_13.csv season file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = pd.read_csv('A_BSA.csv')\n",
        "H = pd.read_csv('H_BSA.csv')"
      ],
      "metadata": {
        "id": "MZuwMUypgsoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the feature vectors to the features\n",
        "X_train = feature_vectors_functions.add_feature_vector(X_train, A, H)\n",
        "results_val = feature_vectors_functions.add_feature_vector(results_val, A, H)\n",
        "results_test = feature_vectors_functions.add_feature_vector(results_test, A, H)"
      ],
      "metadata": {
        "id": "ZxC0lHTJinhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val = results_val.drop('FTR', axis=1)\n",
        "y_val = results_val['FTR']"
      ],
      "metadata": {
        "id": "Pwp5kRWKhB4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the paper the highest accuracy was achieved with 3 PCA components. So I used 3 components as well."
      ],
      "metadata": {
        "id": "7hlCISuZ8p3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use all the data available, the validation season and the first half of the testing season was added to the training data."
      ],
      "metadata": {
        "id": "4owjCkBR8vK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some rounds in the beginning are ignored, this is the correct index\n",
        "# of the start of the second half of the season\n",
        "start_test_index = 13 * matches_per_round"
      ],
      "metadata": {
        "id": "uABl96Ui7f1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_to_append, y_test_to_append = testing_functions.prepare_test_to_append(results_test,\n",
        "                                                                              start_test_index)"
      ],
      "metadata": {
        "id": "s1ZeXcpi7hWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For evaluation on the testing dataset I divided the testing data into rounds approximately. The model is trained on the training dataset, then it is evaluated on one round of the testing dataset and this round is added to the training dataset."
      ],
      "metadata": {
        "id": "DSW0cjAq80bG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding validation season and 1st half of testing season to the training data\n",
        "X_train = pd.concat([X_train, X_val, X_test_to_append])\n",
        "y_train = pd.concat([y_train, y_val, y_test_to_append])\n",
        "# Rounds of the testing dataset\n",
        "slices = testing_functions.get_slices(results_test, matches_per_round,\n",
        "                                      start_test_index)\n",
        "weighted_sum = 0\n",
        "sum = 0\n",
        "for slc in slices:\n",
        "  pca = PCA(n_components=3)\n",
        "  X_train_pca = pca.fit_transform(X_train)\n",
        "  clf = GaussianNB().fit(X_train_pca, y_train)\n",
        "  X_test = slc.drop(['FTR', 'Date'], axis=1)\n",
        "  y_test = slc['FTR']\n",
        "  X_test_pca = pca.transform(X_test)\n",
        "  weighted_sum += (clf.score(X_test_pca, y_test) * len(y_test))\n",
        "  sum += len(y_test)\n",
        "  # Add the round to the training dataset\n",
        "  X_train = pd.concat([X_train, X_test])\n",
        "  y_train = pd.concat([y_train, y_test])\n",
        "print(weighted_sum / sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUDoz4H4hGBP",
        "outputId": "6cb4f4db-947a-4c5d-aef5-6bd8ebdb3c2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4631578947368421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The testing accuracy is 46.32%"
      ],
      "metadata": {
        "id": "zG-Xmpgm9EWK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L94EWTinA4Gg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}