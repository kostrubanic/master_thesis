{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of baseline using PCA and Naive Bayes with Original + Win-loss + feature vectors feature set and SA"
      ],
      "metadata": {
        "id": "yCmOWNhD7xzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necessary files: testing_functions.py, pca_nb_functions.py, feature_vectors_functions.py, SA_06.csv, SA_07.csv, ..., SA_19.csv, A_SA.csv, H_SA.csv"
      ],
      "metadata": {
        "id": "6-mFeXDApDoP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxuUnLs-fWQ1"
      },
      "outputs": [],
      "source": [
        "import testing_functions\n",
        "import pca_nb_functions\n",
        "import feature_vectors_functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The baseline is implemented according to the paper Predicting The Dutch Football Competition Using Public Data: A Machine Learning Approach by N. Tax and Y. Joustra from 2015. The original features from paper and feature vectors are used and the model is evaluated on the SA dataset."
      ],
      "metadata": {
        "id": "tLnc0g8q7_DY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The authors used data from 13 seasons, so I used data from 13 seasons as well. I used seasons from 2006/07 to 2016/17 for training, season 2017/18 for validation and the second half of season 2018/19 for testing. Here the validation part is not necessary, but I wanted to keep the split of the dataset as similar as possible with other baselines and models."
      ],
      "metadata": {
        "id": "q78adEMl8H6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = pca_nb_functions.create_data(['SA_17.csv', 'SA_16.csv',\n",
        "                             'SA_15.csv', 'SA_14.csv', 'SA_13.csv',\n",
        "                             'SA_12.csv', 'SA_11.csv', 'SA_10.csv',\n",
        "                             'SA_09.csv', 'SA_08.csv',\n",
        "                                'SA_07.csv',\n",
        "                                'SA_06.csv'], pca_nb_functions.teams_sa, return_names=True)\n",
        "results_val, matches_per_round = pca_nb_functions.create_data_single('SA_18.csv', 'SA_17.csv',\n",
        "                            ['SA_16.csv',\n",
        "                             'SA_15.csv', 'SA_14.csv', 'SA_13.csv',\n",
        "                             'SA_12.csv', 'SA_11.csv', 'SA_10.csv',\n",
        "                             'SA_09.csv', 'SA_08.csv',\n",
        "                                'SA_07.csv',\n",
        "                                'SA_06.csv'], pca_nb_functions.teams_sa, return_names=True)\n",
        "# Dates are returned as well for dividing testing season into slices\n",
        "results_test, matches_per_round = pca_nb_functions.create_data_single('SA_19.csv', 'SA_18.csv',\n",
        "                            ['SA_17.csv', 'SA_16.csv',\n",
        "                             'SA_15.csv', 'SA_14.csv', 'SA_13.csv',\n",
        "                             'SA_12.csv', 'SA_11.csv', 'SA_10.csv',\n",
        "                             'SA_09.csv', 'SA_08.csv',\n",
        "                                'SA_07.csv',\n",
        "                                'SA_06.csv'], pca_nb_functions.teams_sa,\n",
        "                             return_dates=True, return_names=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqD7Ua0pgTrm",
        "outputId": "399effdb-61f3-4e32-c4f6-039b508f663e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing SA_17.csv season file.\n",
            "Processing SA_16.csv season file.\n",
            "Processing SA_15.csv season file.\n",
            "Processing SA_14.csv season file.\n",
            "Processing SA_13.csv season file.\n",
            "Processing SA_12.csv season file.\n",
            "Processing SA_11.csv season file.\n",
            "Processing SA_10.csv season file.\n",
            "Processing SA_09.csv season file.\n",
            "Processing SA_08.csv season file.\n",
            "Processing SA_07.csv season file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = pd.read_csv('A_SA.csv')\n",
        "H = pd.read_csv('H_SA.csv')"
      ],
      "metadata": {
        "id": "MZuwMUypgsoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the feature vectors to the features\n",
        "X_train = feature_vectors_functions.add_feature_vector(X_train, A, H)\n",
        "results_val = feature_vectors_functions.add_feature_vector(results_val, A, H)\n",
        "results_test = feature_vectors_functions.add_feature_vector(results_test, A, H)"
      ],
      "metadata": {
        "id": "ZxC0lHTJinhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val = results_val.drop('FTR', axis=1)\n",
        "y_val = results_val['FTR']"
      ],
      "metadata": {
        "id": "Pwp5kRWKhB4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the paper the highest accuracy was achieved with 3 PCA components. So I used 3 components as well."
      ],
      "metadata": {
        "id": "DhmL4Oxu8o42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use all the data available, the validation season and the first half of the testing season was added to the training data."
      ],
      "metadata": {
        "id": "uDzgBru38ups"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some rounds in the beginning are ignored, this is the correct index\n",
        "# of the start of the second half of the season\n",
        "start_test_index = 13 * matches_per_round"
      ],
      "metadata": {
        "id": "cgk_59O66zDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_to_append, y_test_to_append = testing_functions.prepare_test_to_append(results_test,\n",
        "                                                                              start_test_index)"
      ],
      "metadata": {
        "id": "vV1t11Ze604K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For evaluation on the testing dataset I divided the testing data into rounds approximately. The model is trained on the training dataset, then it is evaluated on one round of the testing dataset and this round is added to the training dataset."
      ],
      "metadata": {
        "id": "rr29h7UD8zUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding validation season and 1st half of testing season to the training data\n",
        "X_train = pd.concat([X_train, X_val, X_test_to_append])\n",
        "y_train = pd.concat([y_train, y_val, y_test_to_append])\n",
        "# Rounds of the testing dataset\n",
        "slices = testing_functions.get_slices(results_test, matches_per_round,\n",
        "                                      start_test_index)\n",
        "weighted_sum = 0\n",
        "sum = 0\n",
        "for slc in slices:\n",
        "  pca = PCA(n_components=3)\n",
        "  X_train_pca = pca.fit_transform(X_train)\n",
        "  clf = GaussianNB().fit(X_train_pca, y_train)\n",
        "  X_test = slc.drop(['FTR', 'Date'], axis=1)\n",
        "  y_test = slc['FTR']\n",
        "  X_test_pca = pca.transform(X_test)\n",
        "  weighted_sum += (clf.score(X_test_pca, y_test) * len(y_test))\n",
        "  sum += len(y_test)\n",
        "  # Add the round to the training dataset\n",
        "  X_train = pd.concat([X_train, X_test])\n",
        "  y_train = pd.concat([y_train, y_test])\n",
        "print(weighted_sum / sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUDoz4H4hGBP",
        "outputId": "8f520656-f3f1-4aa8-ea31-1c230d96a0f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4421052631578947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The testing accuracy is 44.21%"
      ],
      "metadata": {
        "id": "Bq9JkutE9CKo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pg4_6so0_Gs3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}